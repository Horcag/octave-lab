# 4. ЛАБОРАТОРНАЯ РАБОТА № 4. Численные методы решения задачи Коши для обыкновенных дифференциальных уравнений

## 4.1. Теоретические сведения

### 4.1.1. Постановка задачи

Для простоты изложения основных идей вычислительных методов решения задач с начальными условиями будем рассматривать, как правило, случай одного обыкновенного дифференциального уравнения первого порядка. Обычно эти идеи легко переносятся на системы уравнений первого порядка и сравнительно просто обобщаются на случай уравнений высших порядков.

Пусть на отрезке $x_0 \le x \le X$ требуется найти решение $y(x)$ дифференциального уравнения
$$ y' = f(x, y), \quad (4.1) $$
удовлетворяющее при $x = x_0$ начальному условию
$$ y(x_0) = y_0. \quad (4.2) $$
Условия существования и единственности решения поставленной задачи Коши будем считать выполненными. Будем предполагать также, что функция $f(x, y)$ в некоторой области изменения ее аргументов обладает необходимой по ходу изложения дополнительной гладкостью. Основной нашей целью на данном этапе будет построение вычислительных правил нахождения приближенного решения рассматриваемой задачи.

В вычислительной практике иногда используют аналитический метод, основанный на идее разложения в ряд решения рассматриваемой задачи Коши. Особенно часто для этих целей используют ряд Тейлора. В этом случае вычислительные правила строятся особенно просто. Приближенное решение $y_m(x)$ исходной задачи ищут в виде
$$ y_m(x) = \sum_{k=0}^{m} \frac{(x - x_0)^k}{k!} y^{(k)}(x_0), \quad x_0 \le x \le X, \quad (4.3) $$
где $y^{(0)}(x_0) = y(x_0) = y_0$, $y^1(x_0) = y'(x_0) = f(x_0, y_0)$,
а значения $y^{(k)}(x_0)$, находят по формулам, полученным последовательным дифференцированием уравнения (4.1):
$$ y^{(2)}(x_0) = y''(x_0) = f_x(x_0, y_0) + f(x_0, y_0)f_y(x_0, y_0), $$
$$ y^{(3)}(x_0) = y'''(x_0) = f_{xx}(x_0, y_0) + 2f(x_0, y_0)f_{xy}(x_0, y_0) + f^2(x_0, y_0)f_{yy}(x_0, y_0) + \quad (4.4) $$
$$ + f_y(x_0, y_0)[f_x(x_0, y_0) + f(x_0, y_0)f_y(x_0, y_0)], $$
$$ \dots \dots \dots \dots \dots \dots \dots $$
$$ f_m(x, y_x, y_y, y_{x^2}, f_{xy}, f_{y^2}, \dots , f_{x^{m-1}}, \dots , f_{y^{m-1}})|_{x=x_0, y=y_0} $$
(конкретный вид многочлена $F_m$ не приведен из-за громоздкой записи). Для значений $x$, близких к $x_0$, метод рядов (4.3) при достаточно большом $m$ дает обычно хорошее приближение к точному решению $y(x)$ задачи (4.1), (4.2).
Однако с увеличением расстояния $|x - x_0|$ погрешность приближенного равенства $y(x) \approx y_m(x)$, вообще говоря, возрастает по абсолютной величине и правило (4.3) становится вовсе неприемлемым, когда $x$ выходит из области сходимости соответствующего (4.3) ряда Тейлора.

### 4.1.2. Построение одношаговых методов

Предпочтительными в таких случаях будут, например, численные методы решения задачи Коши, позволяющие в некоторых попарно близких друг другу фиксированных точках (узлах)
$$ x_0 < x_1 < \dots < x_N = X \quad (4.5) $$
последовательно находить значения $y_n \approx y(x_n)$, $n = 1, 2, \dots , N$, приближенного решения.
Сходимость методов подобного типа не так жестко связана с длиной отрезка $[x_0, X]$, и их чаще кладут в основу стандартных программ для ЭВМ. Таким методам ниже будет уделено основное внимание.

Большинство численных методов решения рассматриваемой задачи Коши можно привести к виду
$$ y_{n+1} = F(y_{n-q}, y_{n-q+1}, \dots , y_n, y_{n+1}, \dots , y_{n+s}), \quad (4.6) $$
где $F$ – некоторая известная функция указанных аргументов, определяемая способом построения метода и зависящая от вида уравнения (4.1) и избранной сетки (4.5). При $q = 0, 0 \le s \le 1$ такие вычислительные правила обычно называют **одношаговыми**, а при $q = 1$ или $s > 1$ – **многошаговыми**. Как одношаговые, так и многошаговые методы вида (4.6) называют **явными** в случае $s = 0$ и **неявными** при $s = 1$.
В случае $s > 1$ многошаговые правила часто называют методами **с забеганием вперед**.

Будем считать, что процесс решения задачи (4.1), (4.2) доведен до точки $x_n$ ($0 \le n < N$), и известно (точно или приближенно) соответствующее значение $y(x_n)$ искомого решения. Построим вычислительное правило для нахождения значения решения в очередной узловой точке $x_{n+1} = x_n + h_n$ сетки (4.5). Поскольку при построении одношаговых методов используется информация о решаемой задаче лишь в пределах одного шага интегрирования, то можно без ущерба для понимания не писать индекс, означающий номер шага процесса.

Чтобы по известному значению $y(x)$ соответствующего решения дифференциального уравнения (4.1) в узловой точке $x \ge x_0$ найти значение этого решения в очередной точке $x+h$ сетки (4.5), можно, очевидно, воспользоваться вычислительным правилом типа (4.3), положив там $x_0$ и взяв вместо текущей точки $x$ отрезка $[x_0, X]$ узловую точку $x+h$. Это позволит записать приближенное равенство
$$ y_m(x) \approx \sum_{k=0}^{m} \frac{h^k}{k!} y^{(k)}(x), \quad (4.7) $$
которое может быть положено в основу соответствующего одношагового метода, если для вычисления значений $y^{(i)}(x)$, $i = 2, 3, \dots , m$, использовать формулы типа (4.4). При условии, что данное решение уравнения (4.1) имеет на рассматриваемом отрезке непрерывную производную порядка $m+1$, погрешность приближенного равенства (4.7) будет, очевидно, величиной порядка $h^{m+1}$ и при малых $h > 0$ и больших $m$ построенный пошаговый вариант метода рядов будет давать, как правило, достаточно хорошее приближение к искомому значению решения. Привлекательной чертой полученного вычислительного метода является то обстоятельство, что искомое значение приближенного решения разложено по последовательным главным частям. Это позволяет в процессе решения задачи без дополнительных вычислительных затрат по величине последних слагаемых суммы (4.7) составить представление о локальной погрешности приближенного решения (погрешности нахождения значения $y(x + h)$ в предположении, что значение $y(x)$ известно точно). Однако такой одношаговый метод интегрирования дифференциальных уравнений при $m > 1$ все же редко используется в практике вычислений, так как его применение требует на каждом шаге нахождения значений $m(m+1)/2$ различных функций $f, f_x, f_y, f_{x^2}, f_{xy}, f_{y^2}, \dots , f_{x^{m-1}}, \dots , f_{y^{m-1}}$. При использовании ЭВМ это сопряжено с написанием большого числа блоков вычисления значений указанных функций, что осложняет связь пользователя с машиной и увеличивает, как правило, время решения задачи. Поэтому данный метод редко кладут в основу стандартных программ решения задач Коши, хотя в специальных частных случаях (например, когда приходится многократно решать задачи, отличающиеся лишь начальными данными или мало отличающиеся правыми частями уравнений) использование метода может быть оправданным.

Естественно поставить задачу о таком усовершенствовании приведенного выше одношагового метода, которое сохраняло бы основные его достоинства, но не было связано с нахождением значений производных правой части уравнения (4.1). Чтобы выполнить последнее условие, производные, $y^{(i)}(x)$, $i = 2, 3, \dots , m$, входящие в правую часть равенства (4.1), можно заменить по формулам численного дифференцирования их приближенными выражениями через значения функции $y'$ и учесть, что $y'(x) = f[x, y(x)]$. Требование одношаговости конструируемых правил накладывает при этом свои условия на такую замену. Ниже на конкретных примерах будет рассмотрен один из возможных подходов к решению поставленной задачи.

В случае $m = 1$ приближенное равенство (4.7) не требует вычисления производных правой части уравнения (4.1) и позволяет с погрешностью порядка $h^2$ находить значение $y(x_n + h)$ решения этого уравнения по известному его значению $y(x_n)$. Соответствующее одношаговое правило можно записать в виде
$$ y(x_n + h) = y(x_n) + h f(x_n, y_n). \quad (4.8) $$
Правило (4.8) впервые было построено Эйлером и носит его имя. Иногда его называют также **правилом ломаных** или **методом касательных**, чем подчеркивают простой геометрический смысл формулы. Геометрическая интерпретация одного шага метода Эйлера заключается в аппроксимации решения на отрезке $[x_n, x_{n+1}]$ касательной
$$ y = y(x_n) + y'(x_n)(x - x_n), $$
проведенной в точке $(x_n, y_n)$, к интегральной кривой, проходящей через эту точку. Таким образом, после выполнения $N$ шагов интегральная кривая заменяется ломаной линией (ломаной Эйлера).
Погрешность $r_{n+1}$ этой формулы можно, очевидно, записать в виде
$$ r_{n+1} = \frac{h^2}{2} y''(\bar{x}_n + \theta h), \quad 0 < \theta < 1. $$
При $m=2$ приближенное равенство (4.7) требует вычисления производной $y''(x_n)$ и дает возможность находить значение $y(x_n + h)$ с локальной ошибкой порядка $h^3$. Чтобы не понизить порядок погрешности приближенного равенства
$$ y(x_n + h) = y(x_n) + h y'(x_n) + \frac{h^2}{2} y''(x_n), \quad (4.9) $$
значение производной $y''(x_n)$ необходимо найти по крайней мере не хуже, чем с ошибкой порядка $h$, для чего, очевидно, достаточно иметь два значения функции
$y'(x) = f[x, y(x)]$ из отрезка $x_n \le x \le x_n + h$ ($0 < h < 1$).
В точке $x_n$ значением $y_n$ этой функции мы уже располагаем, так как по предположению, на предыдущем шаге процесса было найдено $y_n$. Найдем теперь еще значение функции $y'$ в точке $x_n + h^i$ ($i \ge 1$). Для этого с учетом уравнения (4.1) достаточно указать правило вычисления $y(x_n + h^i)$.
Очевидно, $y(x_n + h^i) = y(x_n) + h^i y'(x_n) + O(h^{2i})$. Поэтому справедлива следующая расчетная формула
$$ y(x_n + h^i) = y(x_n) + h^i f(x_n, y_n). \quad (4.10) $$
Так как
$$ y''(x_n) = \frac{y'(x_n + h^i) - y'(x_n)}{h^i} + O(h^i), $$
$$ y'(x_n + h^i) = f[x_n + h^i, y(x_n + h^i)], $$
то на основании (4.9) можно записать
$$ y(x_n + h) = y(x_n) + h f(x_n, y_n) + \frac{h^{2-i}}{2} (f[x_n + h^i, y(x_n + h^i)] - f(x_n, y_n)). \quad (4.11) $$
Формулы (4.10), (4.11) можно рассматривать как семейство (зависящих от параметра $i \ge 1$) одношаговых методов решения задачи Коши (4.1), (4.2) с локальной погрешностью порядка $h^3$.
При $i=1$ эти формулы принимают вид
$$ y(x_n + h) = y(x_n) + h f(x_n, y_n) + O(h^2). \quad (4.12) $$
$$ y(x_n + h) = y(x_n) + h f(x_n, y_n) + \frac{h}{2} (f[x_n + h, y(x_n + h)] - f(x_n, y_n)) + O(h^3). \quad (4.13) $$
Они имеют предсказывающе-исправляющий характер: формула (4.12) служит для получения грубого приближения искомой величины $y(x_n+h)$, а по формуле (4.13) производится уточнение полученного значения. Сравнение $y(x_n+h)$ формул (4.12) и (4.13), дает возможность судить о локальной погрешности результата.

Заметим, что иногда на основе формулы (4.13) бывает полезным сделать одну итерацию. Это может (при сохранении порядка погрешности) несколько повысить точность приближения величины $y(x_n + h)$. К существенному увеличению объема вычислений эта итерация не приведет, так как значение $f_{n+1}$, которое при этом необходимо будет вычислить, можно использовать в качестве $f_n$ на следующем этапе вычислений.
В случае $i=2$ формулы (4.10), (4.11) имеют вид
$$ y(x_n + h^2) = y(x_n) + h^2 f(x_n, y_n) + O(h^4). \quad (4.14) $$
$$ y(x_n + h) = y(x_n) + h f(x_n, y_n) + \frac{1}{2} (f[x_n + h^2, y(x_n + h^2)] - f(x_n, y_n)) + O(h^4). \quad (4.15) $$
Увеличение значения $i$ на единицу позволило, вообще говоря, несколько улучшить структуру остаточного члена вычислительного правила. Если в случае правила (4.12), (4.13) погрешность складывалась из одного слагаемого вида $\frac{h^3}{6} y'''(x_n + \theta h)$, $0 < \theta < 1$, представляющего собой ошибку приближенного равенства (4.9), и еще двух слагаемых также порядка $h^3$, порождаемых соответственно неточностью замены производной $y''$ и ошибкой формулы Эйлера (4.10). То в случае (4.14), (4.15) два последних слагаемых остаточного члена будут уже, очевидно, величинами порядка $h^4$. При $i=3$ соответствующие слагаемые остатка станут величинами порядка $h^5$ и т. д. Однако, увеличение значения $i$, улучшая структурные свойства остаточного члена вычислительного правила, предъявляет повышенные требования к выполнению вычислений, так как в этом случае в силу ограниченности разрядной сетки ЭВМ возможна потеря точности результата за счет операций вычитания близких величин и деления на малые по абсолютному значению числа.
Описанным способом можно строить вычислительные методы более высокого порядка точности. Следует, однако, заметить что при больших значениях $m$ построение таких методов связано с приближенной заменой производных высоких порядков по интерполяционным формулам численного дифференцирования. Эта процедура, как известно сопряжена обычно с повышенными требованиями к точности выполнения вычислений.
Рассмотрим другие способы получения одношаговых методов, которые непосредственно не связаны с подобной аппроксимацией производных.

### 4.1.3 Метод Рунге-Кутты

Изложим основную идею этого способа на примере задачи (4.1), (4.2). Интегрируя уравнение (4.1) в пределах от $x$ до $x+h$ ($0 < h < 1$), получим равенство
$$ y(x+h) = y(x) + \int_{x}^{x+h} f[t, y(t)] dt, \quad (4.16) $$
которое посредством последнего интеграла связывает значения решения рассматриваемого уравнения в двух точках, удаленных друг от друга на расстояние шага $h$. Указав эффективный метод приближенного вычисления интеграла в (4.16), мы получим тем самым одно из правил численного интегрирования уравнения (4.1).
Чтобы построить методы более высокого порядка точности, не связанные с вычислением производных функции $f[x, y(x)]$, нужно привлечь дополнительную информацию по значениям интегрируемой функции. Поскольку эта функция, как функция одного аргумента $x$, в случае задачи Коши (4.1), (4.2), в отличие от ее частного случая – рассмотренной уже нами ранее задачи вычисления неопределенного интеграла, – вообще говоря, неизвестна внутри отрезка интегрирования $[x, x+h]$, то использовать непосредственно квадратурные формулы с числом узлов $N > 2$ не удается. Поэтому в способе Рунге-Кутты применяют следующий специальный метод приближенного вычисления интеграла в (4.16).
Для удобства записи используем обозначение $\Delta y = y(x+h) - y(x)$ и равенству (4.16) придадим новый вид, произведя замену переменной интегрирования $t = x + \alpha h$
$$ \Delta y = h \int_{0}^{1} f[x + \alpha h, y(x + \alpha h)] d\alpha, \quad (4.17) $$
Чтобы на основе (4.17) построить одношаговый метод численного интегрирования уравнения (4.1), введем три набора параметров:
$$ \alpha_1, \alpha_2, \dots , \alpha_q; \quad (\alpha) $$
$$ \beta_{10}; \quad (\beta) $$
$$ \beta_{20}, \beta_{21}, $$
$$ \dots \dots \dots $$
$$ \beta_{q0}, \beta_{q1}, \dots , \beta_{q,q-1}; $$
$$ A_0, A_1, \dots . , A_q, \quad (A) $$
выбором которых распорядимся в дальнейшем. При помощи двух первых наборов составим величины
$$ \varphi_0 = h f(x, y), $$
$$ \varphi_1 = h f(x + \alpha_1 h, y + \beta_{10} \varphi_0), $$
$$ \varphi_2 = h f(x + \alpha_2 h, y + \beta_{20} \varphi_0 + \beta_{21} \varphi_1), $$
$$ \dots \dots \dots $$
$$ \varphi_q = h f(x + \alpha_q h, y + \beta_{q0} \varphi_0 + \beta_{q1} \varphi_1 + \dots + \beta_{q,q-1} \varphi_{q-1}), $$
которые при заданных $(\alpha)$ и $(\beta)$ могут быть вычислены последовательно. Хотя
$\varphi_i = h f(x + \alpha_i h, y + \beta_{i0} \varphi_0 + \dots + \beta_{i,i-1} \varphi_{i-1})$, $i = 1, 2, \dots , q$,
вообще говоря, не равны значениям $h f[x + \alpha_i h, y(x + \alpha_i h)]$, однако при соответствующем выборе параметров $(\beta)$ их можно трактовать как приближенные значения интегрируемой функции $f[x + \alpha h, y(x + \alpha h)]$, умноженные на $h$. Это дает основание надеяться при помощи параметров $(A)$ составить такую линейную комбинацию величин $\varphi_i$, $i = 1, 2, \dots , q$, которая будет являться аналогом квадратурной суммы и позволит вычислить приближенное значение приращения $\Delta y$:
$$ \Delta y \approx \sum_{i=0}^{q} A_i \varphi_i. \quad (4.18) $$
Тем самым параметрам $(\alpha)$, $(\beta)$ и $(A)$ можно придать некоторый квадратурный смысл.
Рассмотрим теперь задачу выбора этих параметров. Введем величину
$$ r_q(h) = \Delta y - \sum_{i=0}^{q} A_i \varphi_i. \quad (4.19) $$
представляющую собой погрешность приближенного равенства (4.18). В предположении, что правая часть уравнения (4.1) является достаточно гладкой функцией, запишем следующее разложение этой величины:
$$ r_q(h) = \sum_{j=0}^{k} \frac{h^j}{j!} r_q^{(j)}(0) + \frac{h^{k+1}}{(k+1)!} r_q^{(k+1)}(\theta h), \quad 0 < \theta < 1. $$
На основании этого разложения можно утверждать, что если параметры $(\alpha)$, $(\beta)$ и $(A)$ подобрать так, чтобы выполнялись условия
$$ r_q^{(j)}(0) = 0, \quad j = 0,1, \dots , k, \quad (4.20) $$
то погрешность (4.19) приближенного равенства (4.18) будет величиной порядка не ниже $h^{k+1}$:
$$ r_q(h) = \frac{h^{k+1}}{(k+1)!} r_q^{(k+1)}(\theta h). \quad (4.21) $$
Число $k$ при этом обычно называют **порядком точности** соответствующего метода. Такому определению порядка точности метода можно дать следующее объяснение. В случае рассматриваемой задачи Коши (4.1), (4.2), как и в случае изученной ранее задачи неопределенного интегрирования, оказывается, что, если погрешность типа (4.19) расчетной формулы данного метода является величиной порядка $h^{k+1}$ в достаточно широкой окрестности решения, то погрешность метода (та часть погрешности приближенного решения, которая определяется лишь неточностью самой формулы) для случая конечного отрезка интегрирования будет величиной порядка $h^k$.Доказательство этого факта будет приведено несколько позже, а сейчас продолжим рассмотрение вопроса о построении вычислительных правил по способу Рунге-Кутты.
Для выполнения условий (4.20) при возможно большем значении $k$ величины $r_q^{(j)}(0)$, $j = 0,1, \dots , k$ выражают через значения функции $f(x, y)$ и ее частных производных и требуют обращения в нуль возможно большего числа этих величин для любой достаточно гладкой функции $f$. Иными словами, выбор параметров $(\alpha)$, $(\beta)$ и $(A)$ осуществляют на основании требования, чтобы разложение
$$ \Delta y = y(x+h) - y(x) = \frac{h}{1!} y'(x) + \frac{h^2}{2!} y''(x) + \frac{h^3}{3!} y'''(x) + \dots \quad (4.22) $$
и разложение по степеням $h$ линейной комбинации $\sum_{i=0}^{q} A_i \varphi_i$ совпадали до членов с возможно более высокими степенями $h$ в случае любой правой части уравнения (4.1).
При произвольном $q$ систему уравнений для определения параметров $(\alpha)$, $(\beta)$ и $(A)$ записать очень трудно. Поэтому мы ограничимся здесь рассмотрением лишь нескольких конкретных примеров построения одношаговых правил указанным способом.

**Метод первого порядка точности.** Зададимся минимальным значением $q=0$, что равнозначно введению лишь одного параметра $A_0$. Приближенное равенство (4.18) в этом случае принимает вид
$$ \Delta y \approx A_0 \varphi_0 = h A_0 f(x, y). $$
И погрешность (4.19) может быть записана следующим образом:
$$ r_0(h) = y(x+h) - y(x) - h A_0 f(x, y). $$
Тогда
$$ r_0'(h) = y'(x+h) - A_0 f(x, y), $$
$$ r_0''(h) = y''(x+h). $$
Так как $r_0''(h)$ не зависит от $A_0$, то уже при $j=2$ условие (4.20) в случае произвольной функции $f$ удовлетворено быть не может. Поэтому $k=1$ и система (4.20) принимает вид $(1-A_0)f(x,y) = 0$.
Отсюда находим, что $A_0 = 1$. Следовательно,
$$ \Delta y \approx h f(x, y) $$
и $r_0(h) = \frac{h^2}{2} y_0''(\theta h) = \frac{h^2}{2} y_0''(x + \theta h)$, $0 < \theta < 1$.
В простейшем случае $q=0$, таким образом, способ Рунге-Кутты приводит известному методу Эйлера (4.8). Квадратурный смысл этого вычислительного правила был уже выяснен.

**Методы второго порядка точности.** При $q=1$ имеем
$$ \Delta y \approx A_0 \varphi_0 + A_1 \varphi_1 = h A_0 f(x, y) + h A_1 f(x + \alpha_1 h, y + \beta_{10} f(x, y)). $$
С целью выбора введенных параметров $\alpha_1, \beta_{10}, A_0, A_1$, разложим $\Delta y$ и $A_0 \varphi_0 + A_1 \varphi_1$ по степеням $h$.
Разложение (4.22) для $\Delta y$ с учетом (4.1) можно записать в виде
$$ \Delta y \approx y(x+h) - y(x) = \quad (4.23) $$
$$ = hf + \frac{h^2}{2} (f_x + f f_y) + \frac{h^3}{6} [f_{xx} + 2 f f_{xy} + f^2 f_{yy} + f_y(f_x + f f_y)] + O(h^4). $$
Используя формулу Тейлора, для линейной комбинации $A_0 \varphi_0 + A_1 \varphi_1$ можно дать следующее представление:
$$ A_0 \varphi_0 + A_1 \varphi_1 = h(A_0 + A_1)f + \quad (4.24) $$
$$ +h^2 A_1(\alpha_1 f_x + \beta_{10} f f_y) + \frac{h^3}{2} A_1[\alpha_1^2 f_{xx} + 2 \alpha_1 \beta_{10} f f_{xy} + \beta_{10}^2 f^2 f_{yy}] + O(h^4). $$
Сравним в разложениях (4.23), (4.24) коэффициенты при $h f, h^2 f_x, h^2 f f_y$. Тем самым на выбор четырех параметров $\alpha_1, \beta_{10}, A_0, A_1$ будут наложены три условия:
$$ A_0 + A_1 = 1, \quad A_1 \alpha_1 = \frac{1}{2}, \quad A_1 \beta_{10} = \frac{1}{2}. $$
Непосредственно из (4.23), (4.24) следует, что в случае $q=1$ для произвольных $f$ нельзя добиться совпадения всех членов с множителем $h^3$ за счет выбора введенных параметров. Поэтому при $q=1$ вычислительные правила типа Рунге-Кутты будут иметь лишь второй порядок точности. Параметры $\alpha_1, \beta_{10}, A_0$ могут быть выражены через коэффициент $A_1$ по формулам
$$ \alpha_1 = \beta_{10} = \frac{1}{2A_1}, \quad A_0 = 1 - A_1. $$
В качестве $A_1$ может быть взято, вообще говоря, произвольное отличное от нуля число. Например, при $A_1 = \frac{1}{2}$ будем иметь
$$ \Delta y = \frac{1}{2} (\varphi_0 + \varphi_1) + O(h^3), \quad (4.25) $$
$$ \varphi_0 = h f(x, y), \quad \varphi_1 = h f(x + h, y + \varphi_0). $$
Вычислительное правило, построенное на основе равенства (4.25), имеет следующий квадратурный смысл. Так как $\varphi_1 = h f(x + h, y + h f') \approx h f(x+h, y(x+h))$, то линейная комбинация $\frac{1}{2} (\varphi_0 + \varphi_1)$ является аналогом квадратурной суммы формулы трапеций при вычислении интеграла в правой части равенства (4.17).
Выбрав $A_1 = 1$, получим еще одно из широко известных вычислительных правил типа Рунге-Кутты второго порядка точности:
$$ \Delta y \approx \varphi_1, \quad (4.26) $$
$$ \varphi_0 = h f(x, y), \quad \varphi_1 = h f(x + h/2, y + \varphi_0/2). $$
Формула (4.26) является, очевидно, аналогом одноточечной квадратурной формулы средних прямоугольников.
Локальная погрешность любого из методов типа Рунге - Кутты второго порядка точности, как следует из разложений (4.23), (4.24), может быть представлена в виде
$$ r_1(h) = \frac{h^3}{6} [f_{xx}(1 - 3\alpha_1^2 A_1) + f f_{xy}(1 - 3 A_1 \beta_{10} \alpha_1) + \quad (4.27) $$
$$ + f^2 f_{yy}(1 - 3 \beta_{10}^2 A_1) + f_y(f_x + f f_y)] + O(h^4). $$
Иногда свободный параметр $A_1$ выбирают так, чтобы в этом представлении можно было обратить в нуль хотя бы часть членов. Например, если учесть, что $A_1 = \beta_{10} = \frac{1}{2A_1}$ и положить $A_1 = \frac{3}{4}$, то правая часть равенства (4.27) существенно упростится:
$$ r_1(h) = \frac{h^3}{6} f_y(f_x + f f_y)] + O(h^4). \quad (4.28) $$
При таком выборе $A_1$ будем иметь
$$ \Delta y \approx \frac{1}{4} (\varphi_0 + 3 \varphi_1), \quad (4.29) $$
$$ \varphi_0 = h f(x, y), \quad \varphi_1 = h f(x + 2h/3, y + 2 \varphi_0/3). $$
**Методы третьего порядка точности.** C повышением требований к точности вычислительных правил типа Рунге-Кутты очень быстро возрастает громоздкость необходимых построений, хотя общая схема таких построений и не претерпевает существенных изменений. Поэтому в случае $q=2$ мы не станем здесь воспроизводить все выкладки и выпишем лишь ту систему уравнений, которым должны удовлетворять параметры $(\alpha)$, $(\beta)$ и $(A)$ в методах типа Рунге-Кутты третьего порядка точности:
$$ A_0 + A_1 + A_2 = 1, \quad A_1 \alpha_1 + A_2 \alpha_2 = \frac{1}{2}, \quad A_1 \alpha_1^2 + A_2 \alpha_2^2 = \frac{1}{3}, \quad (4.30) $$
$$ A_2 \alpha_2 \beta_{21} = \frac{1}{6}, \quad \beta_{20} + \beta_{21} = \alpha_2, \quad \beta_{10} = \alpha_1. $$
Одно из решений этой системы шести уравнений с восемью неизвестными приводит к следующим формулам:
$$ \Delta y \approx \frac{1}{6} (\varphi_0 + 4 \varphi_1 + \varphi_2), \quad (4.31) $$
$$ \varphi_0 = h f(x, y), \quad \varphi_1 = h f(x + h/2, y + \varphi_0/2), \quad \varphi_2 = h f(x + h, y - \varphi_0 + 2 \varphi_1). $$
Это вычислительное правило является, очевидно, аналогом трехточечной квадратурной формулы Симпсона. Часто встречается в практике вычислений и такой метод типа Рунге-Кутты третьего порядка точности:
$$ \Delta y \approx \frac{1}{4} (\varphi_0 + 3 \varphi_2), \quad (4.32) $$
$$ \varphi_0 = h f(x, y), \quad \varphi_1 = h f(x + h/3, y + \varphi_0/3), \quad \varphi_2 = h f(x + \frac{2}{3}h, y + \frac{2}{3} \varphi_1). $$
По квадратурному смыслу приведенное вычислительное правило сходно с методом (4.29) второго порядка точности и еще раз косвенно подчеркивает достоинства последнего.
**Методы четвертого порядка точности.** В случае $q=3$ одним из методов будет аналог четырехточечной квадратурной формулы «трех восьмых»:
$$ \Delta y \approx \frac{1}{8} (\varphi_0 + 3 \varphi_1 + 3 \varphi_2 + \varphi_3), \quad (4.33) $$
$$ \varphi_0 = h f(x, y), \quad \varphi_1 = h f(x + \frac{h}{3}, y + \frac{\varphi_0}{3}), $$
$$ \varphi_2 = h f(x + \frac{2}{3}h, y - \frac{\varphi_0}{3} + \varphi_1), \quad \varphi_3 = h f(x + h, y - \varphi_0 - \varphi_1 + \varphi_2). $$
Особенно широко известно другое вычислительное правило типа Рунге-Кутты четвертого порядка точности:
$$ \Delta y \approx \frac{1}{6} (\varphi_0 + 2 \varphi_1 + 2 \varphi_2 + \varphi_3), \quad (4.34) $$
$$ \varphi_0 = h f(x, y), \quad \varphi_1 = h f(x + \frac{h}{2}, y + \frac{\varphi_0}{2}), $$
$$ \varphi_2 = h f(x + \frac{h}{2}, y + \frac{\varphi_1}{2}), \quad \varphi_3 = h f(x + h, y + \varphi_2). $$

### 4.1.4. Многошаговые методы. Экстраполяционный и интерполяционный методы Адамса

Методы, использующие информацию о решаемой задаче на отрезке длиной более одного шага, называются **многошаговыми**. Эти методы имеют вычислительное правило вида
$$ y_{n+1} = \sum_{i=0}^{p} a_i y_{n-i} + h \sum_{i=-s}^{q} A_i f(x_{n-i}, y_{n-i}), \quad (4.35) $$
позволяющие искать приближенное значение $y_{n+1}$ решения рассматриваемой задачи в точке $x_{n+1}$ сетки $x_0 < x_1 < x_2 < \dots < x_N$ в виде линейной комбинации нескольких известных значений $y_{n-i}$ решения в точках $x_{n-i}$ этой сетки с коэффициентами $a_i$ и нескольких приближенных значений $f(x_{n-i}, y_{n-i})$ производной $y'(x) = f(x, y(x))$ искомого решения в точках $x_{n-i}$ с коэффициентами $h A_i$. При этом среди указанных значений производной могут быть и неизвестные (при $s \ge 1$).
Если $s < 1$, то вычислительные методы вида (4.35) обычно называют **явными**,
при $s = 1$ – **неявными** ($A_{-1} \neq 0$),
при $s > 1$ − **методы с забеганием вперед**.
Наиболее применимыми в вычислительной практике являются правила вида
$$ y_{n+1} = y_n + h \sum_{i=-s}^{q} A_i f(x_{n-i}, y_{n-i}). \quad (4.36) $$
При $s = 0$ − экстраполяционный метод Адамса.
При $s = 1$ − интерполяционный метод Адамса.

**Экстраполяционные методы Адамса.**
$$ y_{n+1} = y_n + h \sum_{i=0}^{q} A_i f(x_{n-i}, y_{n-i}). \quad (4.37) $$
Если
$$ h \int_{0}^{1} f(x_n + \alpha h, y(x_n + \alpha h)) d\alpha \quad (4.38) $$
заменить на $h \sum_{i=0}^{q} A_i f(x_{n-i} + \alpha_i h, y(x_{n-i} + \alpha_i h))$, то коэффициенты $A_i, \alpha_i$ необходимо выбирать следующим образом
$$ \sum_{i=0}^{q} A_i = 1, \quad \sum_{i=0}^{q} A_i \alpha_i^j = \frac{1}{j+1}, \quad j = 1,2, \dots , q-1. $$
Для (4.37) $\alpha_i = -i$, $i = 0,1, \dots , q$.
$$ \sum_{i=0}^{q} A_i = 1, \quad \sum_{i=0}^{q} A_i (-i)^j = \frac{1}{j+1}, \quad j = 1,2, \dots , q. \quad (4.39) $$
Так как определитель этой системы есть определитель Вандермонда, а все $\alpha_i = -i$, $i = 0,1, \dots , q$ различны, то значения параметров $A_i$, $i = 0,1, \dots , q$ могут быть выбраны для любого $q \ge 0$ и притом единственным образом.
При заданном $q$ тем самым будет построен соответствующий экстраполяционный метод Адамса (4.37).

Формула локальной погрешности:
$$ r_{n+1} = y(x_n + h) - y(x_n) - h \sum_{i=0}^{q} A_i f(x_n + \alpha_i h, y(x_n + \alpha_i h)), $$
$$ r_{n+1} = h^{k+1} y^{(k+1)}(x_n) \left[ \frac{1}{(k+1)!} - \frac{1}{k!} \sum_{i=0}^{q} A_i \alpha_i^k \right] + O(h^{k+2}). $$
Так как для нашего метода $k = q+1$
$$ r_{n+1} = h^{q+2} y^{(q+2)}(x_n) \left[ \frac{1}{(q+2)!} - \frac{1}{(q+1)!} \sum_{i=0}^{q} A_i \alpha_i^{q+1} \right] + O(h^{q+3}). $$
$q=0: r_{n+1} = \frac{1}{2} h^2 y''(x_n) + O(h^3);$
$q=1: r_{n+1} = \frac{5}{12} h^3 y'''(x_n) + O(h^4).$

Замена уравнения задачи Коши уравнением экстраполяционного метода Адамса приводит к некорректной задаче, так как задание лишь одного начального данного $y_0$ при $q > 0$ не выделяет единственного решения этого разностного уравнения порядка $q+1$. Поэтому достаточно задать дополнительно к $y_0$ значения $y_1, y_2, \dots , y_q$.
Для их нахождения можно использовать любой из рассматриваемых выше одношаговых методов. Часто, чтобы не нарушать однородность вычислительного процесса, конструируют специальные вычислительные алгоритмы, стараясь по возможности более тесно привязать их к методу основного счета.

**Интерполяционные методы Адамса.**
$$ y_{n+1} = y_n + h \sum_{i=-1}^{q} A_i f(x_{n-i}, y_{n-i}). \quad (4.40) $$
Аналогично случаю экстраполяционных формул положив $\alpha_i = -i$, $i = -1,0,1, \dots , q$, получим систему уравнений
$$ \sum_{i=-1}^{q} A_i = 1, \quad \sum_{i=-1}^{q} A_i (-i)^j = \frac{1}{j+1}, \quad j = 1,2, \dots , q+1. \quad (4.41) $$
Формула локальной погрешности для (4.40)
$$ r_{n+1} = h^{q+3} y^{(q+3)}(x_n) \left[ \frac{1}{(q+3)!} - \frac{1}{(q+2)!} \sum_{i=0}^{q} A_i \alpha_i^{q+2} \right] + O(h^{q+4}). \quad (4.42) $$
Приведем интерполяционных методов Адамса (4.40)
$q = -1: y_{n+1} = y_n + h f(x_{n+1}, y_{n+1}),$
$r_{n+1} = -\frac{1}{2} h^2 y''(x_n) + O(h^3),$
$q = 0: y_{n+1} = y_n + \frac{h}{2} (f(x_{n+1}, y_{n+1}) + f(x_n, y_n)),$
$r_{n+1} = -\frac{1}{12} h^3 y'''(x_n) + O(h^4).$

Заметим, что построенные интерполяционные методы Адамса не дают явных выражений для нахождения $y_{n+1}$, а представляют собой уравнение относительно этой неизвестной. Обычно в качестве начального приближения к $y_{n+1}$берут соответствующее значение, полученное экстраполяционным методом Адамса. При этом часто ограничиваются лишь одной итерацией. В этом случае процесс приобретает предсказывающе-исправляющий характер. По формуле экстраполяционного метода Адамса находят приближенное значение $y_{n+1}$ с локальной погрешностью $h^{q+2}$, которое затем уточняется на порядок с помощью интерполяционной формулы Адамса. Такая организация вычислений применяется наиболее часто.

### 4.1.5. Решение линейных граничных задач

Наряду с задачами Коши для обыкновенных дифференциальных уравнений рассматриваются также граничные задачи. В этих задачах дополнительные условия, присоединенные к дифференциальным уравнениям, задаются в виде уравнений, содержащих комбинации значений решения и его производных, взятых в нескольких точках отрезка, на котором ищется решение.
Рассмотрим линейные граничные задачи для дифференциальных уравнений 2-го порядка.
Пусть при $a \le x \le b$ рассматривается граничная задача для дифференциального уравнения
$$ L(y) \equiv y'' + p(x)y' + q(x)y = f(x) \quad (4.43) $$
с условиями
$$ l_a(y) \equiv \alpha_0 y(a) + \alpha_1 y'(a) = A, \quad (4.44) $$
$$ l_b(y) \equiv \beta_0 y(b) + \beta_1 y'(b) = B. \quad (4.45) $$
Будем считать, что граничная задача (4.43 - 4.45) имеет единственное решение, это решение непрерывно на $[a, b]$ и имеет непрерывные производные на этом отрезке до четвертого порядка включительно.

**Метод сеток.**
1. Область задания дифференциального уравнения (4.43) – отрезок $[a, b]$ заменяется некоторой дискретной сеточной областью. Это означает, что на отрезке $[a, b]$ выбирается некоторая система точек. Совокупность этих точек называется **сеткой**. Если положение каждой точки определяется по правилу $x_k = a + k h$, $k = 0,1, \dots , N$, $h = \frac{b-a}{N}$, $N$ − целое число, то сетку называют **равномерной**. Точки $x_k$ называют **узлами сетки**.
2. Граничная задача (4.43 – 4.45) на множестве узлов, принадлежащих сетке, заменяется некоторой сеточной задачей. Под термином сеточная задача мы будем понимать некоторые соотношения между приближенными значениями решения граничной задачи (4.43 – 4.45) в узлах сетки. В рассматриваемом случае это будет система линейных алгебраических уравнений.
3. Полученная сеточная задача решается по какому-либо численному методу и тем самым находятся приближенные значения решения граничной задачи в узлах сетки. Это и является конечной целью метода сеток.

**Методы замены обыкновенных дифференциальных уравнений и граничных условий системой алгебраических уравнений.**
Возвратимся к краевой задаче (4.43 – 4.45). Выберем равномерную сетку: $x_k = a + k h$, $k = 0,1, \dots , N$, $h = \frac{b-a}{N}$.
Дифференциальное уравнение (4.43) будем рассматривать только во внутренних узлах сетки, т.е. будем полагать, что $x = x_k$, $k = 1,2, \dots , N-1$. Граничные условия (4.44 – 4.45) рассмотрим при $x_0 = a, x_N = b$.
Положим в (4.43) $x = x_k$:
$$ L(y)|_{x=x_k} \equiv y''(x_k) + p(x_k)y'(x_k) + q(x_k)y(x_k) = f(x_k), \quad k = 1,2, \dots , N-1. \quad (4.46) $$
Выразим $y'(x_k), y''(x_k)$ через значения функции $y(x)$ в узлах $x_{k-1}, x_k, x_{k+1}$, т.е. через значения $y(x_{k-1}), y(x_k), y(x_{k+1})$. Для этой цели воспользуемся формулами численного дифференцирования.
Имеем
$$ y'(x_k) = \frac{y(x_k) - y(x_{k-1})}{h} + r_k^{(1)}(h), \quad r_k^{(1)}(h) = \frac{h}{2} y''(x_k^{(1)}), \quad x_{k-1} < x_k^{(1)} < x_k; \quad (4.47) $$
$$ y'(x_k) = \frac{y(x_{k+1}) - y(x_k)}{h} + r_k^{(2)}(h), \quad r_k^{(2)}(h) = -\frac{h}{2} y''(x_k^{(2)}), \quad x_{k-1} < x_k^{(2)} < x_k; \quad (4.48) $$
$$ y'(x_k) = \frac{y(x_{k+1}) - y(x_{k-1})}{2h} + r_k^{(3)}(h), \quad r_k^{(3)}(h) = -\frac{h^2}{6} y'''(x_k^{(3)}), \quad x_{k-1} < x_k^{(3)} < x_k; \quad (4.49) $$
$$ y''(x_k) = \frac{y(x_{k+1}) - 2y(x_k) + y(x_{k-1})}{h^2} + r_k^{(4)}(h), \quad (4.50) $$
$$ r_k^{(4)}(h) = -\frac{h^2}{12} y^{IV}(x_k^{(4)}), \quad x_{k-1} < x_k^{(4)} < x_k. $$
Подставив в (4.46) выражения (4.49) и (4.50) для $y'(x_k), y''(x_k)$, получим
$$ L(y)|_{x=x_k} \equiv L_h(y(x_k)) + R_k(h) = f(x_k), \quad (4.51) $$
$$ L_h(y(x_k)) = \frac{y(x_{k+1}) - 2y(x_k) + y(x_{k-1})}{h^2} + p(x_k)\frac{y(x_{k+1}) - y(x_{k-1})}{2h} + q(x_k)y(x_k), $$
$$ R_k(h) = r_k^{(4)}(h) + p(x_k)r_k^{(3)}(h). $$
Выражение $L_h(y(x_k))$ называется **разностным оператором** второго порядка, а величина $R_k(h)$ − **погрешностью аппроксимации** дифференциального оператора $L(y)$ разностным оператором $L_h(y(x_k))$ на решении. Если для $R_k(h)$ выполняется условие
$$ |R_k(h)| \le M h^2, \quad k = 1,2, \dots , N-1, $$
где $M = \text{const}$, не зависящая от $h$, то говорят, что разностный оператор $L_h$ аппроксимирует на решении дифференциальный оператор $L$ с погрешностью второго порядка относительно $h$.
Пусть $h$ достаточно мало, тогда в формуле (4.51) величиной $R_k(h)$ можно пренебречь и мы получим
$$ L_h(y_k) = f(x_k), \quad k = 1,2, \dots , N-1, \quad (4.52) $$
где при выполнении некоторых условий можно предположить, что $y_k \approx y(x_k)$, $k = 0,1, \dots , N$.
Равенство (4.52) будем называть **разностной схемой**, аппроксимирующей уравнение $L(y) = f(x)$.
Отметим еще, что (4.52) есть система линейных алгебраических уравнений, число таких уравнений $N-1$, а матрица этой системы – трехдиагональная. Неизвестными являются $y_0, y_1, \dots , y_N$. Число этих неизвестных в системе равно $N+1$.

Обратимся к граничным условиям (4.44), (4.45). Используя (4.48) при $k=0$, из (4.44) получим
$$ l_a(y) \equiv l_a^{(h)}(y(x_0)) + R_0(h) = A, \quad (4.53) $$
$$ l_a^{(h)}(y(x_0)) \equiv \alpha_0 y(x_0) + \alpha_1 \left(\frac{y(x_1) - y(x_0)}{h}\right), $$
$$ R_0(h) = \alpha_1 r_0^{(2)}(h). $$
Аналогично, используя (4.47) при $k=N$, из (4.45) получим
$$ l_b(y) \equiv l_b^{(h)}(y(x_N)) + R_N(h) = B, \quad (4.54) $$
где
$$ l_b^{(h)}(y(x_N)) \equiv \beta_0 y(x_N) + \beta_1 \left(\frac{y(x_N) - y(x_{N-1})}{h}\right), $$
$$ R_N(h) = \beta_1 r_N^{(1)}(h). $$
При достаточно малом $h$ величинами $R_0(h)$, $R_N(h)$, имеющими первый порядок малости относительно $h$, в выражениях (4.53), (4.54) можно пренебречь. Тогда вместо (4.53), (4.54) будем иметь
$$ l_a^{(h)}(y_0) = A, \quad (4.55) $$
$$ l_b^{(h)}(y_N) = B. \quad (4.56) $$
Операторы $l_a^{(h)}(y_0)$ и $l_b^{(h)}(y_N)$ аппроксимируют соответственно граничные операторы $l_a(y)$ и $l_b(y)$ с погрешностью $O(h)$.
Формулы (4.52), (4.55), (4.56) образуют в совокупности систему $N+1$ линейных алгебраических уравнений с неизвестными $y_0, y_1, \dots , y_N$. В методе сеток эту систему решают обычно методом прогонки и после этого полагают что $y_k \approx y(x_k)$, $k = 0,1, \dots , N$.
Обратим внимание на то, что граничные условия (4.44), (4.45) при необходимости можно аппроксимировать разностными условиями с погрешностью второго порядка малости относительно $h$. Для этого достаточно, например, воспользоваться вместо (4.47), (4.48) следующими формулами:
$$ y'(x_0) = \frac{-y(x_2) + 4y(x_1) - 3y(x_0)}{2h} + O(h^2), $$
$$ y'(x_N) = \frac{3y(x_N) - 4y(x_{N-1}) + y(x_{N-2})}{2h} + O(h^2). $$
В этом случае соответствующая методу прогонки структура матрицы коэффициентов СЛАУ еще должна быть создана. Усложнения, сопутствующие второму варианту, могут быть оправданы тем, что в этом случае исходная дифференциальная краевая задача полностью аппроксимируется алгебраической системой относительно компонент каркаса решения с точностью $O(h^2)$, в то время как о первом варианте такого сказать нельзя (если, конечно, речь не идет о первой краевой задаче, т.е. о случае $\alpha_1 = \beta_1 = 0$, когда в аппроксимации краевых условий вообще нет нужды). Однако в поисках компромисса между качеством аппроксимации и численной устойчивостью при решении конкретных задач первый вариант может оказаться и более предпочтительным.

**Устойчивость конечно-разностной схемы решения краевой задачи.**
Рассмотрим построенные разностные уравнения относительно приближенных значений решения $y_i \approx y(x_i)$, $i = 1, \dots , N-1$:
$$ \frac{y_{i+1} - 2y_i + y_{i-1}}{h^2} + p_i \frac{y_{i+1} - y_{i-1}}{2h} + q_i y_i = f_i. \quad (4.57) $$
После приведения подобных слагаемых в (4.57) получаем стандартное трехточечное разностное уравнение второго порядка
$$ \left(1 + \frac{h}{2} p_i\right) y_{i+1} - (2 - h^2 q_i)y_i + \left(1 - \frac{h}{2} p_i\right) y_{i-1} = h^2 f_i, \quad (4.58) $$
где $i = 1,2, \dots , N-1$.
Остановимся теперь на вопросе устойчивости построенной конечноразностной схемы решения краевой задачи (4.43 – 4.45). Эту устойчивость можно связать с устойчивостью метода прогонки, что в свою очередь, можно гарантировать, когда матрица коэффициентов имеет свойство диагонального преобладания. Посмотрим с этой точки зрения на i-е «внутреннее» уравнение системы, т.е. на уравнение (4.58).
Условие диагонального преобладания для (4.58) означает, что должно выполняться неравенство
$$ |2 - h^2 q_i| > \left|1 + \frac{h}{2} p_i\right| + \left|1 - \frac{h}{2} p_i\right| \quad \forall i = 1,2, \dots , N-1. \quad (4.59) $$
Рассмотрим, что представляет собой правая часть этого неравенства. Раскрывая модули, имеем
$$ \left|1 + \frac{h}{2} p_i\right| + \left|1 - \frac{h}{2} p_i\right| = \begin{cases} -h p_i, & \text{если } h p_i < 2, \\ 2, & \text{если } |h p_i| \le 2, \\ h p_i, & \text{если } h p_i > 2. \end{cases} $$
Следовательно, правую часть неравенства (4.59) как функцию переменной $h p_i$ (считая ее изменяющейся непрерывно) в условных координатах можно представить в виде графика, изображенного на рис. 4.1.

*(Рис. 4.1. Условный график правой части неравенства (4.59))*

Так как левая часть неравенства (4.59) при $q_i > 0$ и малых $h > 0$ (малость $h$ нужна из требований аппроксимации) меньше 2, то на устойчивость прогонки можно рассчитывать лишь в случае, когда $q(x) < 0$. При этом имеет место
$$ |2 - h^2 q_i| = 2 - h^2 q_i > 2 \quad \forall h. $$
Чтобы в таком случае неравенство (4.59) выполнялось при любых $p(x)$, для правой части его считаем допустимым только значение 2 (т.е. используем горизонтальную часть графика на рис. 4.1). Отсюда получаем ограничение
$$ |h p_i| \le 2. $$
Означающее, что устойчивость прогонки можно гарантировать при условии, что шаг дискретизации $h$ удовлетворяет неравенству
$$ h \le \frac{2}{|p_i|}, \quad \forall i = 1,2, \dots , N-1. $$
Усиливая это неравенство и используя утверждение «Аппроксимация плюс устойчивость дает сходимость», приходим к заключению, что если в дифференциальном уравнении (4.43)
$$ q(x) < 0 \quad \forall x \in [a, b]. \quad (4.60) $$
А в определяющем методе конечных разностей разностном уравнении (4.58)
$$ h \le \frac{2}{\max_{x \in [a,b]} |p(x)|}, $$
то метод конечных разностей сходится (по крайней мере, к решению первой краевой задачи, т.е. когда в (4.44), (4.45) $\alpha_1 = \beta_1 = 0$; в других случаях требуется более детальный анализ).
Наличие ограничения на шаг $h$ в методе конечных разностей второго порядка (4.57) характеризует его как условно устойчивый метод. Если отказаться от аппроксимации все производных с порядком $O(h^2)$ и использовать в роли $y'(x_k)$ правые или левые разностные отношения первого порядка точности, связывая их выбор со знаком $p_i$, а именно, рассматривая вместо (4.58) разностное уравнение
$$ \frac{y_{i+1} - 2y_i + y_{i-1}}{h^2} + p_i \begin{cases} \frac{y_{i+1} - y_i}{h}, & \text{если } p_i > 0 \\ \frac{y_i - y_{i-1}}{h}, & \text{если } p_i < 0 \end{cases} + q_i y_i = f_i, $$
при $i = 1,2, \dots , N-1$, придем к конечно-разностному методу
$$ \begin{cases} y_{i-1} - (2 + h p_i - h^2 q_i)y_i + (1 + h p_i)y_{i+1} = h^2 f_i, & \text{если } p_i > 0, \\ (1 - h p_i)y_{i-1} - (2 - h p_i - h^2 q_i)y_i + y_{i+1} = h^2 f_i, & \text{если } p_i < 0, \end{cases} \quad (4.61) $$
имеющему первый порядок точности независимо от точности аппроксимации краевых условий.
Легко видеть, что при условии (4.60) диагональное преобладание будет при любой величине шага $h > 0$. Отсюда следует его **безусловная устойчивость**, правда в ущерб точности; последнее означает необходимость проведения вычислений с более мелким шагом для доведения погрешности решения до некоторой фиксированной величины, чем это требует метод второго порядка (4.58), если он оба одновременно применимы.

**Правило Рунге.**
Укажем практический прием, позволяющий на основе вычислений судить о том, с какой точностью получены приближенные сеточные значения решения.
Пусть $y(x)$ – точное решение некоторой граничной задачи;
А $y_h(x)$ – приближенное решение этой задачи, полученное по методу сеток с шагом $h$.
$$ \varepsilon_h = \frac{y_h(x) - y_{2h}(x)}{2^p - 1}, \quad (4.62) $$
где $p > 0$ − порядок аппроксимации.
Формула (4.62) называется **правилом Рунге**.

**Формирование системы линейных уравнений и решение ее методом прогонки**
$$ \begin{cases} b_0 y_0 + c_0 y_1 = d_0, \\ \dots \\ a_i y_{i-1} + b_i y_i + c_i y_{i+1} = d_i, & i = 1, \dots , N-1, \\ \dots \\ a_N y_{N-1} + b_N y_N = d_N. \end{cases} $$
где $a_i = 1 - \frac{h}{2}p_i$, $b_i = h^2 q_i - 2$, $c_i = 1 + \frac{h}{2}p_i$, $d_i = h^2 f_i$,
$b_0 = h \alpha_0 - \alpha_1$, $c_0 = \alpha_1$, $d_0 = A h$,
$a_N = -\beta_1$, $b_N = h \beta_0 + \beta_1$, $d_N = B h$.
Прогоночные коэффициенты в прямом ходе определяются с помощью выражений:
$$ A_0 = -\frac{c_0}{b_0}, \quad B_0 = \frac{d_0}{b_0}. $$
$$ A_i = \frac{-c_i}{b_i + a_i A_{i-1}}, \quad B_i = \frac{d_i - a_i B_{i-1}}{b_i + a_i A_{i-1}}, \quad i = 1, 2, \dots , N-1, $$
$$ A_N = 0, \quad B_N = \frac{d_N - a_N B_{N-1}}{b_N + a_N A_{N-1}} = y_N^*. $$
Обратный ход метода прогонки: $y_i = A_i y_{i+1} + B_i, \quad i = N-1, \dots , 0$.

## 4.2. Задание

Решается задача Коши: $y' = f(x, y)$, $y(x_0) = y_0$ на отрезке $[a, b]$.
1. Найти шаг интегрирования для решения задачи Коши методом Рунге-Кутты 4-го порядка с точностью $10^{-4}$.
2. Решить методами: Рунге-Кутты 4-го порядка, Эйлера.
3. Найти точное решение задачи Коши аналитическим методом, сравнить его с приближенными. Найти максимум модуля отклонений в узловых точках приближенного решения от точного.
4. Построить на одном графике интегральные кривые точного решения и решений, полученных в п.2.
5. Записать результаты расчетов в сводную таблицу 4.1.

| $x$ | $y$ | $y_{Эйлера}$ | $\Delta y_{Эйлера}$ | $y_{Рунге-Кутты}$ | $\Delta y_{Рунге-Кутты}$ |
|---|---|---|---|---|---|

1. $y' + xy = (1 + x)e^{-x}y^2$, $y(0) = 1$, $[0, 1]$.
2. $xy' + y = 2y^2 \ln x$, $y(1) = \frac{1}{2}$, $[1, 2]$.
3. $2(xy' + y) = xy^2$, $y(1) = 2$, $[1, 2]$.
4. $y' + 4x^3 y = 4(x^3 + 1)e^{-4x}y^2$, $y(0) = 1$, $[0, 1]$.
5. $xy' - y = -y^2 (\ln x + 2) \ln x$, $y(1) = 1$, $[1, 2]$.
6. $y' + xy = (1 + x)e^{-x}y^2$, $y(0) = 1$, $[0, 1]$.
7. $xy' + y = 2y^2 \ln x$, $y(1) = \frac{1}{2}$, $[1, 2]$.
8. $2(xy' + y) = xy^2$, $y(1) = 2$, $[1, 2]$.
9. $y' + 4x^3 y = 4(x^3 + 1)e^{-4x}y^2$, $y(0) = 1$, $[0, 1]$.
10. $xy' - y = -y^2 (\ln x + 2) \ln x$, $y(1) = 1$, $[1, 2]$.
11. $y' + xy = (1 + x)e^{-x}y^2$, $y(0) = 1$, $[0, 1]$.
12. $xy' + y = 2y^2 \ln x$, $y(1) = \frac{1}{2}$, $[1, 2]$.
13. $2(xy' + y) = xy^2$, $y(1) = 2$, $[1, 2]$.
14. $y' + 4x^3 y = 4(x^3 + 1)e^{-4x}y^2$, $y(0) = 1$, $[0, 1]$.
15. $xy' - y = -y^2 (\ln x + 2) \ln x$, $y(1) = 1$, $[1, 2]$.
16. $y' + xy = (1 + x)e^{-x}y^2$, $y(0) = 1$, $[0, 1]$.
17. $xy' + y = 2y^2 \ln x$, $y(1) = \frac{1}{2}$, $[1, 2]$.
18. $2(xy' + y) = xy^2$, $y(1) = 2$, $[1, 2]$.
19. $y' + 4x^3 y = 4(x^3 + 1)e^{-4x}y^2$, $y(0) = 1$, $[0, 1]$.
20. $xy' - y = -y^2 (\ln x + 2) \ln x$, $y(1) = 1$, $[1, 2]$.
21. $y' + xy = (1 + x)e^{-x}y^2$, $y(0) = 1$, $[0, 1]$.
22. $xy' + y = 2y^2 \ln x$, $y(1) = \frac{1}{2}$, $[1, 2]$.
23. $2(xy' + y) = xy^2$, $y(1) = 2$, $[1, 2]$.
24. $y' + 4x^3 y = 4(x^3 + 1)e^{-4x}y^2$, $y(0) = 1$, $[0, 1]$.
25. $xy' - y = -y^2 (\ln x + 2) \ln x$, $y(1) = 1$, $[1, 2]$.

---